{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nomadic\n",
    "All rights reserved - 2024\n",
    "\n",
    "## Overview\n",
    "This is an example of using Nomadic to perform a hyperparameter search on an LLM pipeline.\n",
    "\n",
    "In this example, we perform hyperparameter search over a basic RAG pipeline. This pipeline\n",
    "takes in the Llama 2 academic paper, answers questions on the document, and measures a \n",
    "correctness metric. We investigate tuning the following parameters:\n",
    "- Chunk size\n",
    "- Top k value\n",
    "\n",
    "We perform this hyperparameter search with either Nomadic's default ParamTuner or \n",
    "RayTuneParamTuner, which uses Ray Tune for hyperparameter optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "from llama_index.core import (\n",
    "    Document,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.evaluation.eval_utils import (\n",
    "    get_responses,\n",
    "    aget_responses,\n",
    ")\n",
    "from llama_index.core.evaluation import (\n",
    "    SemanticSimilarityEvaluator,\n",
    "    BatchEvalRunner,\n",
    "    QueryResponseDataset\n",
    ")\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.readers.file import PDFReader\n",
    "import numpy as np\n",
    "from ray import tune\n",
    "\n",
    "from nomadic.tune.tuner import ParamTuner, RayTuneParamTuner\n",
    "from nomadic.tune.tuner import RunResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mustafa's OpenAI key, use sparingly please.\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-proj-gSjHA2Ve0MwmGbo5KcPuT3BlbkFJwbGxbpmjK22mQmXNgwhZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Directory: /Users/mustafabal/Dev/AutoParamOptimization/samples/llm_pipeline\n"
     ]
    }
   ],
   "source": [
    "# Boilerplate code for sample run\n",
    "PROJECT_DIRECTORY = f\"{(os.path.abspath(''))}\"\n",
    "print(f\"Project Directory: {PROJECT_DIRECTORY}\")\n",
    "OPENAI_ENV = os.getenv('OPENAI_API_KEY')\n",
    "if not OPENAI_ENV:\n",
    "    raise KeyboardInterrupt(\"The OpenAI environment variable is not set. Please configure env variable `OPENAI_API_KEY`.\")\n",
    "    \n",
    "\n",
    "# Helper Functions\n",
    "def execute_bash_command(command: str) -> str:\n",
    "    result = subprocess.run(command, shell=True, check=True, text=True, capture_output=True)\n",
    "    return result.stdout\n",
    "\n",
    "def _build_index(chunk_size, docs):\n",
    "    index_out_path = f\"{PROJECT_DIRECTORY}/data/storage_{chunk_size}\"\n",
    "    if not os.path.exists(index_out_path):\n",
    "        Path(index_out_path).mkdir(parents=True, exist_ok=True)\n",
    "    if os.listdir(index_out_path) == 0:\n",
    "        # parse docs\n",
    "        node_parser = SimpleNodeParser.from_defaults(chunk_size=chunk_size)\n",
    "        base_nodes = node_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "        # build index\n",
    "        index = VectorStoreIndex(base_nodes)\n",
    "        # save index to disk\n",
    "        index.storage_context.persist(index_out_path)\n",
    "    else:\n",
    "        # rebuild storage context\n",
    "        storage_context = StorageContext.from_defaults(\n",
    "            persist_dir=index_out_path\n",
    "        )\n",
    "        # load index\n",
    "        index = load_index_from_storage(\n",
    "            storage_context,\n",
    "        )\n",
    "    return index\n",
    "\n",
    "\n",
    "def _get_eval_batch_runner():\n",
    "    evaluator_s = SemanticSimilarityEvaluator(embed_model=OpenAIEmbedding())\n",
    "    eval_batch_runner = BatchEvalRunner(\n",
    "        {\"semantic_similarity\": evaluator_s}, workers=2, show_progress=True\n",
    "    )\n",
    "\n",
    "    return eval_batch_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function\n",
    "def objective_function(params_dict):\n",
    "    chunk_size = params_dict[\"chunk_size\"]\n",
    "    docs = params_dict[\"docs\"]\n",
    "    top_k = params_dict[\"top_k\"]\n",
    "    eval_qs = params_dict[\"eval_qs\"]\n",
    "    ref_response_strs = params_dict[\"ref_response_strs\"]\n",
    "\n",
    "    # build index\n",
    "    index = _build_index(chunk_size, docs)\n",
    "\n",
    "    # query engine\n",
    "    query_engine = index.as_query_engine(similarity_top_k=top_k)\n",
    "\n",
    "    # get predicted responses\n",
    "    pred_response_objs = get_responses(\n",
    "        eval_qs, query_engine, show_progress=True\n",
    "    )\n",
    "\n",
    "    # run evaluator\n",
    "    # NOTE: can uncomment other evaluators\n",
    "    eval_batch_runner = _get_eval_batch_runner()\n",
    "    eval_results = eval_batch_runner.evaluate_responses(\n",
    "        eval_qs, responses=pred_response_objs, reference=ref_response_strs\n",
    "    )\n",
    "\n",
    "    # get semantic similarity metric\n",
    "    mean_score = np.array(\n",
    "        [r.score for r in eval_results[\"semantic_similarity\"]]\n",
    "    ).mean()\n",
    "\n",
    "    return RunResult(score=mean_score, params=params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm_pipeline_rag(chunk_size_hp_space, top_k_hp_space):\n",
    "    # Obtain RAG inputs: docs, eval_qs, and ref_response_strs\n",
    "    execute_bash_command(f'mkdir -p {PROJECT_DIRECTORY}/data && wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"{PROJECT_DIRECTORY}/data/llama2.pdf\"')\n",
    "    loader = PDFReader()\n",
    "    docs0 = loader.load_data(file=Path(f\"{PROJECT_DIRECTORY}/data/llama2.pdf\"))\n",
    "    doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n",
    "    docs = [Document(text=doc_text)]\n",
    "\n",
    "    execute_bash_command(f'wget \"https://www.dropbox.com/scl/fi/fh9vsmmm8vu0j50l3ss38/llama2_eval_qr_dataset.json?rlkey=kkoaez7aqeb4z25gzc06ak6kb&dl=1\" -O {PROJECT_DIRECTORY}/data/llama2_eval_qr_dataset.json')\n",
    "    eval_dataset = QueryResponseDataset.from_json(\n",
    "        f\"{PROJECT_DIRECTORY}/data/llama2_eval_qr_dataset.json\"\n",
    "    )\n",
    "    eval_qs = eval_dataset.questions\n",
    "    ref_response_strs = [r for (_, r) in eval_dataset.qr_pairs]\n",
    "\n",
    "    # Select tuner and configure hyperparameter search space\n",
    "    fixed_param_dict = {\n",
    "        \"docs\": docs,\n",
    "        \"eval_qs\": eval_qs[:10],\n",
    "        \"ref_response_strs\": ref_response_strs[:10],\n",
    "    }\n",
    "    param_dict = {\n",
    "        \"chunk_size\": tune.grid_search(chunk_size_hp_space),\n",
    "        \"top_k\": tune.grid_search(top_k_hp_space)\n",
    "    }\n",
    "    param_tuner = RayTuneParamTuner(\n",
    "        param_fn=objective_function,\n",
    "        param_dict=param_dict,\n",
    "        fixed_param_dict=fixed_param_dict,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    results = param_tuner.fit()\n",
    "\n",
    "    best_result = results.best_run_result\n",
    "    best_top_k = results.best_run_result.params[\"top_k\"]\n",
    "    best_chunk_size = results.best_run_result.params[\"chunk_size\"]\n",
    "\n",
    "    print(f\"Score: {best_result.score}\")\n",
    "    print(f\"Top-k: {best_top_k}\")\n",
    "    print(f\"Chunk size: {best_chunk_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-05-17 20:52:58</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:59.41        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.5/16.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/10 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 3<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>param_fn_wrapper_fa285_00004</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-17_20-51-56_444047_85614/artifacts/2024-05-17_20-51-58/param_fn_wrapper_2024-05-17_20-51-56/driver_artifacts/param_fn_wrapper_fa285_00004_4_chunk_size=512,top_k=2_2024-05-17_20-51-59/error.txt</td></tr>\n",
       "<tr><td>param_fn_wrapper_fa285_00006</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-17_20-51-56_444047_85614/artifacts/2024-05-17_20-51-58/param_fn_wrapper_2024-05-17_20-51-56/driver_artifacts/param_fn_wrapper_fa285_00006_6_chunk_size=256,top_k=5_2024-05-17_20-51-59/error.txt</td></tr>\n",
       "<tr><td>param_fn_wrapper_fa285_00007</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-17_20-51-56_444047_85614/artifacts/2024-05-17_20-51-58/param_fn_wrapper_2024-05-17_20-51-56/driver_artifacts/param_fn_wrapper_fa285_00007_7_chunk_size=512,top_k=5_2024-05-17_20-51-59/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  chunk_size</th><th style=\"text-align: right;\">  top_k</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   score</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>param_fn_wrapper_fa285_00000</td><td>TERMINATED</td><td>127.0.0.1:85687</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.0394 </td><td style=\"text-align: right;\">0.96348 </td></tr>\n",
       "<tr><td>param_fn_wrapper_fa285_00001</td><td>TERMINATED</td><td>127.0.0.1:85688</td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        36.5124 </td><td style=\"text-align: right;\">0.961268</td></tr>\n",
       "<tr><td>param_fn_wrapper_fa285_00002</td><td>TERMINATED</td><td>127.0.0.1:85689</td><td style=\"text-align: right;\">        1024</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.05061</td><td style=\"text-align: right;\">0.961437</td></tr>\n",
       "<tr><td>param_fn_wrapper_fa285_00003</td><td>TERMINATED</td><td>127.0.0.1:85690</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        38.9305 </td><td style=\"text-align: right;\">0.967294</td></tr>\n",
       "<tr><td>param_fn_wrapper_fa285_00005</td><td>TERMINATED</td><td>127.0.0.1:85692</td><td style=\"text-align: right;\">        1024</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.12885</td><td style=\"text-align: right;\">0.965045</td></tr>\n",
       "<tr><td>param_fn_wrapper_fa285_00008</td><td>TERMINATED</td><td>127.0.0.1:85695</td><td style=\"text-align: right;\">        1024</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        55.8979 </td><td style=\"text-align: right;\">0.976147</td></tr>\n",
       "<tr><td>param_fn_wrapper_fa285_00004</td><td>ERROR     </td><td>127.0.0.1:85691</td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>param_fn_wrapper_fa285_00006</td><td>ERROR     </td><td>127.0.0.1:85693</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>param_fn_wrapper_fa285_00007</td><td>ERROR     </td><td>127.0.0.1:85694</td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|█         | 1/10 [00:01<00:12,  1.34s/it]\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.07it/s]\n",
      "\u001b[36m(param_fn_wrapper pid=85688)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 0.8242452576384002 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 59832, Requested 492. Please try again in 324ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "\u001b[36m(param_fn_wrapper pid=85688)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 0.8934591746451639 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 59832, Requested 492. Please try again in 324ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "\u001b[36m(param_fn_wrapper pid=85688)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 0.33603632563448327 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 59905, Requested 651. Please try again in 556ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "\u001b[36m(param_fn_wrapper pid=85688)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5248893599875476 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 59903, Requested 657. Please try again in 560ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "\u001b[36m(param_fn_wrapper pid=85688)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 0.4217084039678297 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 59882, Requested 674. Please try again in 556ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "\u001b[36m(param_fn_wrapper pid=85688)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 0.8632460569993364 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 59863, Requested 689. Please try again in 552ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "\u001b[36m(param_fn_wrapper pid=85688)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 0.2051118094370603 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 59879, Requested 693. Please try again in 572ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "\u001b[36m(param_fn_wrapper pid=85688)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 0.7455986091895529 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 59817, Requested 725. Please try again in 542ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "\u001b[36m(param_fn_wrapper pid=85688)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 0.6951375688362026 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 59805, Requested 750. Please try again in 555ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      " 10%|█         | 1/10 [00:02<00:25,  2.86s/it]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5842920354399344 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 59789, Requested 834. Please try again in 623ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "\u001b[36m(param_fn_wrapper pid=85689)\u001b[0m /opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(param_fn_wrapper pid=85689)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      " 10%|█         | 1/10 [00:05<00:45,  5.08s/it]\u001b[32m [repeated 34x across cluster]\u001b[0m\n",
      "100%|██████████| 10/10 [00:01<00:00,  6.91it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(param_fn_wrapper pid=85690)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 0.8008652585737386 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 59687, Requested 640. Please try again in 327ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[36m(param_fn_wrapper pid=85692)\u001b[0m /opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(param_fn_wrapper pid=85692)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 40%|████      | 4/10 [00:04<00:05,  1.08it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.64it/s]\n",
      "\u001b[36m(param_fn_wrapper pid=85690)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 1.8636949298696837 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 59957, Requested 698. Please try again in 655ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\u001b[32m [repeated 70x across cluster]\u001b[0m\n",
      "100%|██████████| 10/10 [00:01<00:00,  7.39it/s]\n",
      "\u001b[36m(param_fn_wrapper pid=85687)\u001b[0m /opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(param_fn_wrapper pid=85687)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[36m(param_fn_wrapper pid=85693)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5668174817322356 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 70%|███████   | 7/10 [00:14<00:06,  2.27s/it]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(param_fn_wrapper pid=85691)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 4.941372587334188 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 59677, Requested 1228. Please try again in 905ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
      "\u001b[36m(param_fn_wrapper pid=85688)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 7.871215334665635 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}.\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      " 50%|█████     | 5/10 [00:11<00:07,  1.58s/it]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(param_fn_wrapper pid=85694)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 3.3073896943899346 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 58923, Requested 2735. Please try again in 1.658s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "2024-05-17 20:52:28,276\tERROR tune_controller.py:1331 -- Trial task failed for trial param_fn_wrapper_fa285_00004\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/_private/worker.py\", line 861, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=85691, ip=127.0.0.1, actor_id=83c67e336b372242d010abe801000000, repr=param_fn_wrapper)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 248, in _trainable_func\n",
      "    output = fn()\n",
      "             ^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/nomadic/tune/tuner.py\", line 112, in param_fn_wrapper\n",
      "    tuned_result = self.param_fn(full_param_dict)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/81/9r0gkcqn2gndsl4nxkzktknw0000gn/T/ipykernel_85614/2792286722.py\", line 16, in objective_function\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/evaluation/eval_utils.py\", line 48, in get_responses\n",
      "    return asyncio_run(aget_responses(*args, **kwargs))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/async_utils.py\", line 32, in asyncio_run\n",
      "    return asyncio.run(coro)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py\", line 190, in run\n",
      "    return runner.run(main)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py\", line 118, in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/evaluation/eval_utils.py\", line 36, in aget_responses\n",
      "    return await asyncio_mod.gather(*tasks)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tqdm/asyncio.py\", line 79, in gather\n",
      "    res = [await f for f in cls.as_completed(ifs, loop=loop, timeout=timeout,\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tqdm/asyncio.py\", line 79, in <listcomp>\n",
      "    res = [await f for f in cls.as_completed(ifs, loop=loop, timeout=timeout,\n",
      "           ^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tqdm/asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py\", line 65, in aquery\n",
      "    query_result = await self._aquery(str_or_query_bundle)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 206, in _aquery\n",
      "    response = await self._response_synthesizer.asynthesize(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py\", line 309, in asynthesize\n",
      "    response_str = await self.aget_response(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py\", line 23, in aget_response\n",
      "    return await super().aget_response(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py\", line 367, in aget_response\n",
      "    response = await self._agive_response_single(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py\", line 486, in _agive_response_single\n",
      "    structured_response = await program.acall(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py\", line 99, in acall\n",
      "    answer = await self._llm.apredict(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py\", line 526, in apredict\n",
      "    chat_response = await self.achat(messages)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py\", line 73, in wrapped_async_llm_chat\n",
      "    f_return_val = await f(_self, messages, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/llms/openai/base.py\", line 598, in achat\n",
      "    return await achat_fn(messages, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 110, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 78, in inner\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/__init__.py\", line 410, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/__init__.py\", line 183, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/llms/openai/base.py\", line 645, in _achat\n",
      "    response = await aclient.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1181, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1790, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1493, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=85691, ip=127.0.0.1, actor_id=83c67e336b372242d010abe801000000, repr=param_fn_wrapper)\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 328, in train\n",
      "    result = self.step()\n",
      "             ^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 104, in step\n",
      "    training_result: Optional[_TrainingResult] = session.get_next()\n",
      "                                                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/train/_internal/session.py\", line 304, in get_next\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/train/_internal/session.py\", line 370, in _report_thread_runner_error\n",
      "    raise StartTraceback from e\n",
      "ray.air._internal.util.StartTraceback\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=85691, ip=127.0.0.1, actor_id=83c67e336b372242d010abe801000000, repr=param_fn_wrapper)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
      "    skipped = skip_exceptions(e)\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/air/_internal/util.py\", line 53, in skip_exceptions\n",
      "    return skip_exceptions(exc.__cause__)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/air/_internal/util.py\", line 56, in skip_exceptions\n",
      "    new_exc = copy.copy(exc).with_traceback(exc.__traceback__)\n",
      "              ^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/copy.py\", line 102, in copy\n",
      "    return _reconstruct(x, None, *rv)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/copy.py\", line 265, in _reconstruct\n",
      "    y = func(*args)\n",
      "        ^^^^^^^^^^^\n",
      "TypeError: APIStatusError.__init__() missing 2 required keyword-only arguments: 'response' and 'body'\n",
      "\u001b[36m(param_fn_wrapper pid=85691)\u001b[0m /opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(param_fn_wrapper pid=85691)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[36m(param_fn_wrapper pid=85694)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 2.4405826370504937 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      " 80%|████████  | 8/10 [00:18<00:03,  1.74s/it]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(param_fn_wrapper pid=85695)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 3.860401560843068 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 59881, Requested 4050. Please try again in 3.931s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "2024-05-17 20:52:34,017\tERROR tune_controller.py:1331 -- Trial task failed for trial param_fn_wrapper_fa285_00006\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/_private/worker.py\", line 861, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=85693, ip=127.0.0.1, actor_id=5d1ed5282f37267f635c360c01000000, repr=param_fn_wrapper)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 248, in _trainable_func\n",
      "    output = fn()\n",
      "             ^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/nomadic/tune/tuner.py\", line 112, in param_fn_wrapper\n",
      "    tuned_result = self.param_fn(full_param_dict)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/81/9r0gkcqn2gndsl4nxkzktknw0000gn/T/ipykernel_85614/2792286722.py\", line 16, in objective_function\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/evaluation/eval_utils.py\", line 48, in get_responses\n",
      "    return asyncio_run(aget_responses(*args, **kwargs))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/async_utils.py\", line 32, in asyncio_run\n",
      "    return asyncio.run(coro)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py\", line 190, in run\n",
      "    return runner.run(main)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py\", line 118, in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/evaluation/eval_utils.py\", line 36, in aget_responses\n",
      "    return await asyncio_mod.gather(*tasks)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tqdm/asyncio.py\", line 79, in gather\n",
      "    res = [await f for f in cls.as_completed(ifs, loop=loop, timeout=timeout,\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tqdm/asyncio.py\", line 79, in <listcomp>\n",
      "    res = [await f for f in cls.as_completed(ifs, loop=loop, timeout=timeout,\n",
      "           ^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tqdm/asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py\", line 65, in aquery\n",
      "    query_result = await self._aquery(str_or_query_bundle)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 206, in _aquery\n",
      "    response = await self._response_synthesizer.asynthesize(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py\", line 309, in asynthesize\n",
      "    response_str = await self.aget_response(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py\", line 23, in aget_response\n",
      "    return await super().aget_response(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py\", line 367, in aget_response\n",
      "    response = await self._agive_response_single(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py\", line 486, in _agive_response_single\n",
      "    structured_response = await program.acall(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py\", line 99, in acall\n",
      "    answer = await self._llm.apredict(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py\", line 526, in apredict\n",
      "    chat_response = await self.achat(messages)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py\", line 73, in wrapped_async_llm_chat\n",
      "    f_return_val = await f(_self, messages, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/llms/openai/base.py\", line 598, in achat\n",
      "    return await achat_fn(messages, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 110, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 78, in inner\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/__init__.py\", line 410, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/__init__.py\", line 183, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/llms/openai/base.py\", line 645, in _achat\n",
      "    response = await aclient.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1181, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1790, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1493, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 58598, Requested 1457. Please try again in 55ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=85693, ip=127.0.0.1, actor_id=5d1ed5282f37267f635c360c01000000, repr=param_fn_wrapper)\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 328, in train\n",
      "    result = self.step()\n",
      "             ^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 104, in step\n",
      "    training_result: Optional[_TrainingResult] = session.get_next()\n",
      "                                                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/train/_internal/session.py\", line 304, in get_next\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/train/_internal/session.py\", line 370, in _report_thread_runner_error\n",
      "    raise StartTraceback from e\n",
      "ray.air._internal.util.StartTraceback\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=85693, ip=127.0.0.1, actor_id=5d1ed5282f37267f635c360c01000000, repr=param_fn_wrapper)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
      "    skipped = skip_exceptions(e)\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/air/_internal/util.py\", line 53, in skip_exceptions\n",
      "    return skip_exceptions(exc.__cause__)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/air/_internal/util.py\", line 56, in skip_exceptions\n",
      "    new_exc = copy.copy(exc).with_traceback(exc.__traceback__)\n",
      "              ^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/copy.py\", line 102, in copy\n",
      "    return _reconstruct(x, None, *rv)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/copy.py\", line 265, in _reconstruct\n",
      "    y = func(*args)\n",
      "        ^^^^^^^^^^^\n",
      "TypeError: APIStatusError.__init__() missing 2 required keyword-only arguments: 'response' and 'body'\n",
      "\u001b[36m(param_fn_wrapper pid=85693)\u001b[0m /opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(param_fn_wrapper pid=85693)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[36m(param_fn_wrapper pid=85693)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 7.385121772142694 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "2024-05-17 20:52:37,011\tERROR tune_controller.py:1331 -- Trial task failed for trial param_fn_wrapper_fa285_00007\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/_private/worker.py\", line 861, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=85694, ip=127.0.0.1, actor_id=43484ea6a781e40e496eec8a01000000, repr=param_fn_wrapper)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 248, in _trainable_func\n",
      "    output = fn()\n",
      "             ^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/nomadic/tune/tuner.py\", line 112, in param_fn_wrapper\n",
      "    tuned_result = self.param_fn(full_param_dict)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/81/9r0gkcqn2gndsl4nxkzktknw0000gn/T/ipykernel_85614/2792286722.py\", line 16, in objective_function\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/evaluation/eval_utils.py\", line 48, in get_responses\n",
      "    return asyncio_run(aget_responses(*args, **kwargs))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/async_utils.py\", line 32, in asyncio_run\n",
      "    return asyncio.run(coro)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py\", line 190, in run\n",
      "    return runner.run(main)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py\", line 118, in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/evaluation/eval_utils.py\", line 36, in aget_responses\n",
      "    return await asyncio_mod.gather(*tasks)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tqdm/asyncio.py\", line 79, in gather\n",
      "    res = [await f for f in cls.as_completed(ifs, loop=loop, timeout=timeout,\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tqdm/asyncio.py\", line 79, in <listcomp>\n",
      "    res = [await f for f in cls.as_completed(ifs, loop=loop, timeout=timeout,\n",
      "           ^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tqdm/asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py\", line 65, in aquery\n",
      "    query_result = await self._aquery(str_or_query_bundle)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 206, in _aquery\n",
      "    response = await self._response_synthesizer.asynthesize(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py\", line 309, in asynthesize\n",
      "    response_str = await self.aget_response(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py\", line 23, in aget_response\n",
      "    return await super().aget_response(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py\", line 367, in aget_response\n",
      "    response = await self._agive_response_single(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py\", line 486, in _agive_response_single\n",
      "    structured_response = await program.acall(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py\", line 99, in acall\n",
      "    answer = await self._llm.apredict(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py\", line 526, in apredict\n",
      "    chat_response = await self.achat(messages)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py\", line 73, in wrapped_async_llm_chat\n",
      "    f_return_val = await f(_self, messages, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/llms/openai/base.py\", line 598, in achat\n",
      "    return await achat_fn(messages, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 110, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 78, in inner\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/__init__.py\", line 410, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/__init__.py\", line 183, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/llama_index/llms/openai/base.py\", line 645, in _achat\n",
      "    response = await aclient.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1181, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1790, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1493, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 58533, Requested 2203. Please try again in 736ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=85694, ip=127.0.0.1, actor_id=43484ea6a781e40e496eec8a01000000, repr=param_fn_wrapper)\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 328, in train\n",
      "    result = self.step()\n",
      "             ^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 104, in step\n",
      "    training_result: Optional[_TrainingResult] = session.get_next()\n",
      "                                                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/train/_internal/session.py\", line 304, in get_next\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/train/_internal/session.py\", line 370, in _report_thread_runner_error\n",
      "    raise StartTraceback from e\n",
      "ray.air._internal.util.StartTraceback\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=85694, ip=127.0.0.1, actor_id=43484ea6a781e40e496eec8a01000000, repr=param_fn_wrapper)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
      "    skipped = skip_exceptions(e)\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/air/_internal/util.py\", line 53, in skip_exceptions\n",
      "    return skip_exceptions(exc.__cause__)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mustafabal/Dev/AutoParamOptimization/.venv/lib/python3.11/site-packages/ray/air/_internal/util.py\", line 56, in skip_exceptions\n",
      "    new_exc = copy.copy(exc).with_traceback(exc.__traceback__)\n",
      "              ^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/copy.py\", line 102, in copy\n",
      "    return _reconstruct(x, None, *rv)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/copy.py\", line 265, in _reconstruct\n",
      "    y = func(*args)\n",
      "        ^^^^^^^^^^^\n",
      "TypeError: APIStatusError.__init__() missing 2 required keyword-only arguments: 'response' and 'body'\n",
      " 90%|█████████ | 9/10 [00:29<00:04,  4.18s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "100%|██████████| 10/10 [00:32<00:00,  3.24s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\u001b[36m(param_fn_wrapper pid=85695)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 3.7151652358955602 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 58624, Requested 4577. Please try again in 3.201s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "100%|██████████| 10/10 [00:01<00:00,  7.10it/s]\n",
      "\u001b[36m(param_fn_wrapper pid=85688)\u001b[0m /opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(param_fn_wrapper pid=85688)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      " 70%|███████   | 7/10 [00:37<00:34, 11.34s/it]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "100%|██████████| 10/10 [00:01<00:00,  6.86it/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\u001b[36m(param_fn_wrapper pid=85695)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 4.8162774835894515 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 55988, Requested 4229. Please try again in 217ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "\u001b[36m(param_fn_wrapper pid=85690)\u001b[0m /opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(param_fn_wrapper pid=85690)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[36m(param_fn_wrapper pid=85695)\u001b[0m Retrying llama_index.llms.openai.base.OpenAI._achat in 3.4799535700098128 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-jWbzImeCSj8rVKdTR1QxzYDG on tokens per min (TPM): Limit 60000, Used 59824, Requested 4577. Please try again in 4.401s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      " 80%|████████  | 8/10 [00:43<00:18,  9.47s/it]\n",
      " 90%|█████████ | 9/10 [00:47<00:07,  7.85s/it]\n",
      "100%|██████████| 10/10 [00:52<00:00,  5.29s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|█         | 1/10 [00:00<00:02,  3.56it/s]\n",
      " 30%|███       | 3/10 [00:00<00:01,  6.95it/s]\n",
      " 40%|████      | 4/10 [00:00<00:00,  7.70it/s]\n",
      " 50%|█████     | 5/10 [00:00<00:00,  7.82it/s]\n",
      " 60%|██████    | 6/10 [00:00<00:00,  7.98it/s]\n",
      " 70%|███████   | 7/10 [00:00<00:00,  8.28it/s]\n",
      " 80%|████████  | 8/10 [00:01<00:00,  7.19it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  7.46it/s]\n",
      "2024-05-17 20:52:58,380\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/Users/mustafabal/ray_results/param_fn_wrapper_2024-05-17_20-51-56' in 0.0195s.\n",
      "2024-05-17 20:52:58,384\tERROR tune.py:1035 -- Trials did not complete: [param_fn_wrapper_fa285_00004, param_fn_wrapper_fa285_00006, param_fn_wrapper_fa285_00007]\n",
      "2024-05-17 20:52:58,384\tINFO tune.py:1039 -- Total run time: 59.42 seconds (59.39 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9761472258435484\n",
      "Top-k: 5\n",
      "Chunk size: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(param_fn_wrapper pid=85695)\u001b[0m /opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(param_fn_wrapper pid=85695)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    }
   ],
   "source": [
    "chunk_size_hp_space = [256, 512, 1024]\n",
    "top_k_hp_space = [1, 2, 5]\n",
    "run_llm_pipeline_rag(chunk_size_hp_space, top_k_hp_space)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
